{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 8(Yicheng Zou).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YichengZou626/COMP590_intro_to_deep_learning/blob/main/Homework_8(Yicheng_Zou).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufVNz7Y6oZPg"
      },
      "source": [
        "# Homework 8\n",
        "\n",
        "In this homework, you will be training a sequence-to-sequence model. Unfortunately, these models can be somewhat expensive to train, and the free GPU/TPU from colab is not sufficient to train a sequence-to-sequence model on a terribly large/difficult task. So, you will be translating short phrases from English to French using a small dataset comes from [Tatoeba](tatoeba.org).\n",
        "\n",
        "This homework contains example mxnet code that mainly comes from the textbook. There is an analogous Pytorch example in the textbook you can use instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL4emIYirepb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d600882b-fff6-4e08-e214-f2c64d4a1ee4"
      },
      "source": [
        "!pip install d2l mxnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-0.17.4-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 461 kB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting d2l\n",
            "  Downloading d2l-0.17.3-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 866 kB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.3\n",
            "  Downloading matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 23.0 MB/s \n",
            "\u001b[?25hCollecting pandas==1.2.2\n",
            "  Downloading pandas-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 26.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (7.7.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3->d2l) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3->d2l) (3.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3->d2l) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3->d2l) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3->d2l) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.2->d2l) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.3.3->d2l) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.3->d2l) (1.15.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.9.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (3.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Installing collected packages: numpy, requests, pandas, matplotlib, graphviz, mxnet, d2l\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.0+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.1 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed d2l-0.17.3 graphviz-0.8.4 matplotlib-3.3.3 mxnet-1.9.0 numpy-1.18.5 pandas-1.2.2 requests-2.25.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ldp-XktrhCl"
      },
      "source": [
        "import collections\n",
        "from d2l import mxnet as d2l\n",
        "import math\n",
        "from mxnet import np, npx, init, gluon, autograd\n",
        "from mxnet.gluon import nn, rnn\n",
        "npx.set_np()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2TnrCXJsaxx"
      },
      "source": [
        "# Example sequence-to-sequence model implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MffMkQz0rqk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae55c5dd-0a5f-49b0-d86d-c9177c8a7e9d"
      },
      "source": [
        "class Seq2SeqEncoder(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
        "        # Embedding layer for looking up embeddings of tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = self.embedding(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.swapaxes(0, 1)\n",
        "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
        "        output, state = self.rnn(X, state)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).swapaxes(0, 1)\n",
        "        output, state = self.rnn(X, state)\n",
        "        output = self.dense(output).swapaxes(0, 1)\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        # `weights` shape: (`batch_size`, `num_steps`, 1)\n",
        "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
        "        # sequence_mask zeros out entries that are beyond the sequence lengths\n",
        "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
        "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n",
        "\n",
        "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [\n",
        "                x.as_in_ctx(device) for x in batch]\n",
        "            bos = np.array(\n",
        "                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n",
        "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            with autograd.record():\n",
        "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "                l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.backward()\n",
        "            # Add gradient clipping\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            trainer.step(num_tokens)\n",
        "            metric.add(l.sum(), num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, loss {(metric[0] / metric[1])}\")\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "    \n",
        "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Add the batch axis\n",
        "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(axis=2)\n",
        "        pred = dec_X.squeeze(axis=0).astype('int32').item()\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)),\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
        "\n",
        "# Generate some sequences!\n",
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "for eng in engs:\n",
        "    translation = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
        "    print(f'{eng} => {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss 0.24009968360921896\n",
            "Epoch 20, loss 0.18821761046090213\n",
            "Epoch 30, loss 0.14967849788493126\n",
            "Epoch 40, loss 0.12620719142524991\n",
            "Epoch 50, loss 0.10735590933627474\n",
            "Epoch 60, loss 0.09367445818508108\n",
            "Epoch 70, loss 0.08294192039793\n",
            "Epoch 80, loss 0.07240383046107797\n",
            "Epoch 90, loss 0.06525398980252906\n",
            "Epoch 100, loss 0.05817260652112004\n",
            "Epoch 110, loss 0.05277993950211406\n",
            "Epoch 120, loss 0.04799309512472021\n",
            "Epoch 130, loss 0.04519115288114041\n",
            "Epoch 140, loss 0.04099128115885561\n",
            "Epoch 150, loss 0.03808096495729316\n",
            "Epoch 160, loss 0.03537184964104555\n",
            "Epoch 170, loss 0.03259612011749976\n",
            "Epoch 180, loss 0.031448045803401846\n",
            "Epoch 190, loss 0.029107022595095945\n",
            "Epoch 200, loss 0.027584049655714638\n",
            "Epoch 210, loss 0.02656651359522075\n",
            "Epoch 220, loss 0.024949619968028483\n",
            "Epoch 230, loss 0.02403260515881823\n",
            "Epoch 240, loss 0.024495033427797307\n",
            "Epoch 250, loss 0.02257408667905553\n",
            "Epoch 260, loss 0.02182258625098074\n",
            "Epoch 270, loss 0.021082518655265798\n",
            "Epoch 280, loss 0.021119177130346736\n",
            "Epoch 290, loss 0.021273414059353363\n",
            "Epoch 300, loss 0.020277114798355554\n",
            "loss 0.020, 14316.6 tokens/sec on cpu(0)\n",
            "go . => ('va !',)\n",
            "i lost . => (\"j'ai perdu .\",)\n",
            "he's calm . => ('il est malade .',)\n",
            "i'm home . => ('je suis chez moi .',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fo0IMZE4Rkt"
      },
      "source": [
        "# Problems\n",
        "\n",
        "1. In the example, sequence-to-sequence learning is enabled by passing the final encoder state to the decoder and using it to initialize the decoder state. In class, we also discussed passing the encoder state to the decoder by concatenating it with the embeddings used as input to the decoder. Implement this. Does this change the loss and/or the predictions of the model? Why do you think this is?\n",
        "1. The prediction function above uses greedy sampling (choosing the most probable token at each time step). Change it so that it instead samples randomly at each time step according to the predicted distribution. Generate the predictions a few times for the example English sentences provided. Do they ever change? Why do you think they do or don't change? Hint: You will need to implement a categorical sampler. One way to do this with numpy is described [here](https://stackoverflow.com/a/62875642).\n",
        "1. In class, we discussed deep RNNs, which consist of stacks of RNN layers. Change the model so that it has two GRU layers in the encoder and two GRU layers in the decoder. Is training faster or slower? Is the final loss higher or lower? Hint: It might help you to use [SequentialRNNCell](https://mxnet.apache.org/versions/1.6/api/python/docs/api/gluon/rnn/index.html#mxnet.gluon.rnn.SequentialRNNCell)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: Concatenating the encoder state with embedding "
      ],
      "metadata": {
        "id": "jTLbmabf52Ng"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTm48QOeUMeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6bc513-9c63-4815-f2a6-fb1cbfa7a092"
      },
      "source": [
        "class Seq2SeqEncoder1(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder1, self).__init__(**kwargs)\n",
        "        # Embedding layer for looking up embeddings of tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = self.embedding(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.swapaxes(0, 1)\n",
        "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
        "        output, state = self.rnn(X, state)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder1(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder1, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).swapaxes(0, 1)\n",
        "        # `context` shape: (`batch_size`, `num_hiddens`)\n",
        "        context = state[0][-1]\n",
        "        # Broadcast `context` so it has the same `num_steps` as `X`\n",
        "        context = np.broadcast_to(context, (\n",
        "            X.shape[0], context.shape[0], context.shape[1]))\n",
        "        X_and_context = np.concatenate((X, context), 2)\n",
        "        output, state = self.rnn(X_and_context, state)\n",
        "        output = self.dense(output).swapaxes(0, 1)\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "\n",
        "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        # `weights` shape: (`batch_size`, `num_steps`, 1)\n",
        "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
        "        # sequence_mask zeros out entries that are beyond the sequence lengths\n",
        "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
        "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n",
        "\n",
        "def train_seq2seq1(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [\n",
        "                x.as_in_ctx(device) for x in batch]\n",
        "            bos = np.array(\n",
        "                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n",
        "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            with autograd.record():\n",
        "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "                l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.backward()\n",
        "            # Add gradient clipping\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            trainer.step(num_tokens)\n",
        "            metric.add(l.sum(), num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, loss {(metric[0] / metric[1])}\")\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "    \n",
        "def predict_seq2seq1(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Add the batch axis\n",
        "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(axis=2)\n",
        "        pred = dec_X.squeeze(axis=0).astype('int32').item()\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)),\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder1(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder1(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq1(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
        "\n",
        "# Generate some sequences!\n",
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "for eng in engs:\n",
        "    translation = predict_seq2seq1(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
        "    print(f'{eng} => {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss 0.23776725669959922\n",
            "Epoch 20, loss 0.19640280007846897\n",
            "Epoch 30, loss 0.16760551267981952\n",
            "Epoch 40, loss 0.143198858889385\n",
            "Epoch 50, loss 0.1261709818263919\n",
            "Epoch 60, loss 0.10868061062671311\n",
            "Epoch 70, loss 0.09596645696010236\n",
            "Epoch 80, loss 0.08349801029383971\n",
            "Epoch 90, loss 0.07486014045255947\n",
            "Epoch 100, loss 0.06805979178112259\n",
            "Epoch 110, loss 0.06097593010996609\n",
            "Epoch 120, loss 0.05507651223577134\n",
            "Epoch 130, loss 0.04916465362194531\n",
            "Epoch 140, loss 0.04519039924948856\n",
            "Epoch 150, loss 0.040913800281222906\n",
            "Epoch 160, loss 0.03818250987906008\n",
            "Epoch 170, loss 0.03571809759669208\n",
            "Epoch 180, loss 0.03314604706672269\n",
            "Epoch 190, loss 0.030880573648770275\n",
            "Epoch 200, loss 0.029393817251566496\n",
            "Epoch 210, loss 0.02757653651487072\n",
            "Epoch 220, loss 0.026061763917306708\n",
            "Epoch 230, loss 0.025329366906896814\n",
            "Epoch 240, loss 0.02470295600936151\n",
            "Epoch 250, loss 0.02391478171830851\n",
            "Epoch 260, loss 0.023320843693967037\n",
            "Epoch 270, loss 0.023095194254743448\n",
            "Epoch 280, loss 0.02216358871640112\n",
            "Epoch 290, loss 0.021543959377788736\n",
            "Epoch 300, loss 0.02139817809833405\n",
            "loss 0.021, 11551.3 tokens/sec on cpu(0)\n",
            "go . => ('va !',)\n",
            "i lost . => (\"j'ai <unk> .\",)\n",
            "he's calm . => ('il est malade .',)\n",
            "i'm home . => (\"je suis refuse m'en m'en m'en m'en m'en m'en suis\",)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss does not change yet, and the reason for that might be this task is really simple, and the encoder state and input X might cover the same information."
      ],
      "metadata": {
        "id": "wxyfVWRk6C4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d8iyCrwNzCH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Using random selection for prediction"
      ],
      "metadata": {
        "id": "lsqzbL386iu2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yny7QwEc7Xco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8566485b-0242-4086-9194-6ec1d09cb55f"
      },
      "source": [
        "class Seq2SeqEncoder2(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder2, self).__init__(**kwargs)\n",
        "        # Embedding layer for looking up embeddings of tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = self.embedding(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.swapaxes(0, 1)\n",
        "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
        "        output, state = self.rnn(X, state)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder2(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder2, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).swapaxes(0, 1)\n",
        "        output, state = self.rnn(X, state)\n",
        "        output = self.dense(output).swapaxes(0, 1)\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        # `weights` shape: (`batch_size`, `num_steps`, 1)\n",
        "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
        "        # sequence_mask zeros out entries that are beyond the sequence lengths\n",
        "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
        "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n",
        "\n",
        "def train_seq2seq2(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [\n",
        "                x.as_in_ctx(device) for x in batch]\n",
        "            bos = np.array(\n",
        "                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n",
        "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            with autograd.record():\n",
        "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "                l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.backward()\n",
        "            # Add gradient clipping\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            trainer.step(num_tokens)\n",
        "            metric.add(l.sum(), num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, loss {(metric[0] / metric[1])}\")\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder2(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder2(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq2(net, train_iter, lr, num_epochs, tgt_vocab, device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss 0.23005600756240238\n",
            "Epoch 20, loss 0.1829521384871602\n",
            "Epoch 30, loss 0.15010172961406565\n",
            "Epoch 40, loss 0.1245324319856742\n",
            "Epoch 50, loss 0.10314159813662863\n",
            "Epoch 60, loss 0.09026079541911576\n",
            "Epoch 70, loss 0.07599551693654727\n",
            "Epoch 80, loss 0.06703110596928152\n",
            "Epoch 90, loss 0.05804091351841998\n",
            "Epoch 100, loss 0.05135159546913288\n",
            "Epoch 110, loss 0.0447427133367269\n",
            "Epoch 120, loss 0.04036101108927841\n",
            "Epoch 130, loss 0.03549119441915525\n",
            "Epoch 140, loss 0.03346643256465547\n",
            "Epoch 150, loss 0.029964339306203458\n",
            "Epoch 160, loss 0.028287501937523923\n",
            "Epoch 170, loss 0.026401704523986554\n",
            "Epoch 180, loss 0.025101357958815032\n",
            "Epoch 190, loss 0.023937299935567195\n",
            "Epoch 200, loss 0.023435795039747794\n",
            "Epoch 210, loss 0.022183061772000832\n",
            "Epoch 220, loss 0.02191745564019009\n",
            "Epoch 230, loss 0.02156379775520574\n",
            "Epoch 240, loss 0.021537907618042616\n",
            "Epoch 250, loss 0.021729603098959775\n",
            "Epoch 260, loss 0.020927067958947384\n",
            "Epoch 270, loss 0.020152349398859556\n",
            "Epoch 280, loss 0.019533741901071933\n",
            "Epoch 290, loss 0.019488819843669052\n",
            "Epoch 300, loss 0.019399316608319363\n",
            "loss 0.019, 12154.1 tokens/sec on cpu(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_seq2seq2(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Add the batch axis\n",
        "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "        Y = np.exp(Y) / np.sum(np.exp(Y), axis=2)\n",
        "        n1,n2,m = Y.shape\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        cum_prob = np.cumsum(Y, axis=2) # shape (n1, n2, m)\n",
        "        r = np.random.uniform(size=(n1, n2, 1))\n",
        "        for i in range(m):\n",
        "          p = cum_prob[:,:,i]\n",
        "          if p>r:\n",
        "            dec_X = np.expand_dims(np.array([i], ctx=device), axis=0)\n",
        "            pred = i\n",
        "            break \n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)),\n",
        "\n",
        "# Generate some sequences!\n",
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "for eng in engs:\n",
        "    translation = predict_seq2seq2(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
        "    print(f'{eng} => {translation}')"
      ],
      "metadata": {
        "id": "2Y2YA5UD_JMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87099d6a-645e-43e7-a4d3-4e19c5f0e6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go . => ('va !',)\n",
            "i lost . => (\"j'ai perdu .\",)\n",
            "he's calm . => ('venez !',)\n",
            "i'm home . => ('je suis chez moi .',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running several times, the predicted output does not change very much. The reason for that might be when we using the cumsum function, the first index that cumsum probability is large than a uniform random value is very similar to the max probability."
      ],
      "metadata": {
        "id": "gqiCkw1P6qWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sK3E-rfluAA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Using two layers of GRU"
      ],
      "metadata": {
        "id": "xWJUcvK47W21"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQEwUxxCWdQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b83a87c-e8df-4182-b227-97e294919f6c"
      },
      "source": [
        "class Seq2SeqEncoder3(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder3, self).__init__(**kwargs)\n",
        "        # Embedding layer for looking up embeddings of tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.SequentialRNNCell()\n",
        "        self.rnn.add(rnn.GRU(num_hiddens, num_layers, dropout=dropout))\n",
        "        self.rnn.add(rnn.GRU(num_hiddens, num_layers, dropout=dropout))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = self.embedding(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.swapaxes(0, 1)\n",
        "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
        "        output, state = self.rnn(X, state)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder3(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder3, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.SequentialRNNCell()\n",
        "        self.rnn.add(rnn.GRU(num_hiddens, num_layers, dropout=dropout))\n",
        "        self.rnn.add(rnn.GRU(num_hiddens, num_layers, dropout=dropout))\n",
        "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).swapaxes(0, 1)\n",
        "        output, state = self.rnn(X, state)\n",
        "        output = self.dense(output).swapaxes(0, 1)\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        # `weights` shape: (`batch_size`, `num_steps`, 1)\n",
        "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
        "        # sequence_mask zeros out entries that are beyond the sequence lengths\n",
        "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
        "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n",
        "\n",
        "def train_seq2seq3(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [\n",
        "                x.as_in_ctx(device) for x in batch]\n",
        "            bos = np.array(\n",
        "                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n",
        "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            with autograd.record():\n",
        "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "                l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.backward()\n",
        "            # Add gradient clipping\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            trainer.step(num_tokens)\n",
        "            metric.add(l.sum(), num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, loss {(metric[0] / metric[1])}\")\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "    \n",
        "def predict_seq2seq3(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Add the batch axis\n",
        "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(axis=2)\n",
        "        pred = dec_X.squeeze(axis=0).astype('int32').item()\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)),\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder3(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder3(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq3(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
        "\n",
        "# Generate some sequences!\n",
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "for eng in engs:\n",
        "    translation = predict_seq2seq3(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
        "    print(f'{eng} => {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss 0.2503205088825969\n",
            "Epoch 20, loss 0.20541966425908076\n",
            "Epoch 30, loss 0.18181233666910518\n",
            "Epoch 40, loss 0.1634592576460405\n",
            "Epoch 50, loss 0.15013314269274908\n",
            "Epoch 60, loss 0.13469427695006003\n",
            "Epoch 70, loss 0.12309898630177492\n",
            "Epoch 80, loss 0.11230183510928583\n",
            "Epoch 90, loss 0.1052308215817238\n",
            "Epoch 100, loss 0.09527665730866332\n",
            "Epoch 110, loss 0.08791289616644453\n",
            "Epoch 120, loss 0.07935538774210371\n",
            "Epoch 130, loss 0.07287428558271544\n",
            "Epoch 140, loss 0.06668792583491097\n",
            "Epoch 150, loss 0.06186449860832157\n",
            "Epoch 160, loss 0.058285869701975496\n",
            "Epoch 170, loss 0.051888693884947504\n",
            "Epoch 180, loss 0.047289984379167496\n",
            "Epoch 190, loss 0.04386471425956837\n",
            "Epoch 200, loss 0.040574314368141774\n",
            "Epoch 210, loss 0.038813990150460664\n",
            "Epoch 220, loss 0.03788162037782995\n",
            "Epoch 230, loss 0.03444385838198971\n",
            "Epoch 240, loss 0.03180265830098902\n",
            "Epoch 250, loss 0.03114714372913371\n",
            "Epoch 260, loss 0.02939308576347414\n",
            "Epoch 270, loss 0.027282814114453144\n",
            "Epoch 280, loss 0.027117239026186363\n",
            "Epoch 290, loss 0.025355568461322447\n",
            "Epoch 300, loss 0.025810891368735357\n",
            "loss 0.026, 7887.5 tokens/sec on cpu(0)\n",
            "go . => ('va !',)\n",
            "i lost . => (\"j'ai perdu .\",)\n",
            "he's calm . => ('réveille-toi !',)\n",
            "i'm home . => ('je suis chez moi .',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss was a little bit larger than the original model, and it trained much slower than original model."
      ],
      "metadata": {
        "id": "AGhK0O1f7cwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-1kRk5Uwv85V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using one layer of GRU for encoder and decoder"
      ],
      "metadata": {
        "id": "Sasy60JdLcMK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc63403-0d0f-4775-a0b8-bcd9689e5e06",
        "id": "Dqu1yp0cLNvT"
      },
      "source": [
        "class Seq2SeqEncoder4(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder4, self).__init__(**kwargs)\n",
        "        # Embedding layer for looking up embeddings of tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = self.embedding(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.swapaxes(0, 1)\n",
        "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
        "        output, state = self.rnn(X, state)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder4(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder4, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
        "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).swapaxes(0, 1)\n",
        "        output, state = self.rnn(X, state)\n",
        "        output = self.dense(output).swapaxes(0, 1)\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        # `weights` shape: (`batch_size`, `num_steps`, 1)\n",
        "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
        "        # sequence_mask zeros out entries that are beyond the sequence lengths\n",
        "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
        "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n",
        "\n",
        "def train_seq2seq4(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [\n",
        "                x.as_in_ctx(device) for x in batch]\n",
        "            bos = np.array(\n",
        "                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n",
        "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            with autograd.record():\n",
        "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "                l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.backward()\n",
        "            # Add gradient clipping\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            trainer.step(num_tokens)\n",
        "            metric.add(l.sum(), num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, loss {(metric[0] / metric[1])}\")\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "    \n",
        "def predict_seq2seq4(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Add the batch axis\n",
        "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(axis=2)\n",
        "        pred = dec_X.squeeze(axis=0).astype('int32').item()\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)),\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 1, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder4(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder4(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq4(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
        "\n",
        "# Generate some sequences!\n",
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "for eng in engs:\n",
        "    translation = predict_seq2seq4(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
        "    print(f'{eng} => {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss 0.21543541840332223\n",
            "Epoch 20, loss 0.1750390249084929\n",
            "Epoch 30, loss 0.15615579454226083\n",
            "Epoch 40, loss 0.14651427255845922\n",
            "Epoch 50, loss 0.13237358800354365\n",
            "Epoch 60, loss 0.12061903074190214\n",
            "Epoch 70, loss 0.11149809671076955\n",
            "Epoch 80, loss 0.10174629867475646\n",
            "Epoch 90, loss 0.0949796431180015\n",
            "Epoch 100, loss 0.08940905278613838\n",
            "Epoch 110, loss 0.08453020623517478\n",
            "Epoch 120, loss 0.07958375014267959\n",
            "Epoch 130, loss 0.07591996123498934\n",
            "Epoch 140, loss 0.07208194342338865\n",
            "Epoch 150, loss 0.0674941642037624\n",
            "Epoch 160, loss 0.06338788899133074\n",
            "Epoch 170, loss 0.060431928908623186\n",
            "Epoch 180, loss 0.056315086339224704\n",
            "Epoch 190, loss 0.053811052660921434\n",
            "Epoch 200, loss 0.05179602421441917\n",
            "Epoch 210, loss 0.04849194147979965\n",
            "Epoch 220, loss 0.04696960122695076\n",
            "Epoch 230, loss 0.04410491164206332\n",
            "Epoch 240, loss 0.04166348737696408\n",
            "Epoch 250, loss 0.04006085101191053\n",
            "Epoch 260, loss 0.0388812219378799\n",
            "Epoch 270, loss 0.036646863880330116\n",
            "Epoch 280, loss 0.03595688435938451\n",
            "Epoch 290, loss 0.034755649926952376\n",
            "Epoch 300, loss 0.03324172610328311\n",
            "loss 0.033, 19223.8 tokens/sec on cpu(0)\n",
            "go . => ('<unk> !',)\n",
            "i lost . => (\"j'ai perdu .\",)\n",
            "he's calm . => ('il est <unk> .',)\n",
            "i'm home . => ('je suis <unk> .',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the num_layer from 2 to 1, the loss will increase and it will train faster!"
      ],
      "metadata": {
        "id": "TZ_CRVK0NZwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6XoTSTXnNhAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}