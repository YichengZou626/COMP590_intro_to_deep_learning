{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 7 (Yicheng Zou).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YichengZou626/COMP590_intro_to_deep_learning/blob/main/Homework_7_(Yicheng_Zou).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2hPDx3Tvh0v"
      },
      "source": [
        "# Homework 6\n",
        "\n",
        "In this homework you will be training and using a \"char-RNN\". This is the name given to a character-level recurrent neural network language model by [this famous blog post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Before you start on the rest of the homework, please give the blog post a read, it's quite good!\n",
        "\n",
        "I don't expect you to implement the char-RNN from scratch. Andrej's original char-rnn is in Torch (the predecessor to PyTorch that is not commonly used anymore). Fortunately, there are many other implementations of this model available; for example, there is one (in both mxnet and pytorch) in chapters 8 and 9 of [the textbook](http://d2l.ai), and another pytorch one [here](https://github.com/spro/char-rnn.pytorch). **Please use one of these example implementations (or another one that you find) when completing this homework**.\n",
        "\n",
        "For this homework, please complete the following steps:\n",
        "\n",
        "1. Download and tokenize the [Shakespeare dataset](http://www.gutenberg.org/files/100/100-0.txt) at a character level. I recommend basing your solution on the following code:\n",
        "```Python\n",
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "raw_dataset = ' '.join(re.sub('[^A-Za-z]+','', text).lower().split())\n",
        "# Maps token index to character\n",
        "idx_to_char = list(set(raw_dataset))\n",
        "# Maps character to token index\n",
        "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "# Tokenize the dataset\n",
        "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
        "```\n",
        "1. Train a \"vanilla\" RNN (as described in chapter 8 of [the textbook](http://d2l.ai)) on the Shakespeare dataset. Report the training loss and generate some samples from the model at the end of training.\n",
        "1. Train a GRU RNN (as described in chapter 9 of [the textbook](http://d2l.ai)) on the Shakespeare datatset. Is the final training loss higher or lower than the vanilla RNN? Are the samples from the model more or less realistic?\n",
        "1. Find a smaller, simpler dataset than the Shakespeare data (you can find some ideas in Andrej's blog post, but feel free to get creative!) and train either the vanilla or GRU RNN on it instead. Is the final training loss higher or lower than it was for the Shakespeare data?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('shake.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "raw_dataset = ' '.join(re.sub('[^A-Za-z ]+','', text).lower().split())\n",
        "# Maps token index to character\n",
        "idx_to_char = list(set(raw_dataset))\n",
        "# Maps character to token index\n",
        "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "# Tokenize the dataset\n",
        "corpus_indices = [char_to_idx[char] for char in raw_dataset]"
      ],
      "metadata": {
        "id": "nSEiuVZixxDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Turning a string into a tensor\n",
        "\n",
        "def char_tensor(string):\n",
        "    c_list = [char_to_idx[char] for char in string]\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(c_list)):\n",
        "        tensor[c] = c_list[c]\n",
        "    return tensor\n",
        "\n",
        "# Readable time elapsed\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "metadata": {
        "id": "s4i0r3ztwkCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "kCYP2pNuwwrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Vanilla RNN"
      ],
      "metadata": {
        "id": "C9tBrMNg6D1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        batch_size = input.size(0)\n",
        "        encoded = self.encoder(input)\n",
        "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
        "        output = self.decoder(output.view(batch_size, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def forward2(self, input, hidden):\n",
        "        encoded = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ],
      "metadata": {
        "id": "xnaKfsMl6Cv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden(1)\n",
        "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
        "\n",
        "    hidden = hidden.to(device)\n",
        "    prime_input = prime_input.to(device)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[:,p], hidden)\n",
        "        \n",
        "    inp = prime_input[:,-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = idx_to_char[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
        "        inp = inp.to(device)\n",
        "\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "Vn_xdG231u7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"shake.txt\"\n",
        "n_epochs=2000\n",
        "print_every=100\n",
        "hidden_size=100\n",
        "n_layers=2\n",
        "learning_rate=0.01\n",
        "chunk_len=200\n",
        "batch_size=100\n",
        "\n",
        "\n",
        "file = raw_dataset\n",
        "file_len = len(file)\n",
        "\n",
        "def random_training_set(chunk_len, batch_size):\n",
        "    inp = torch.LongTensor(batch_size, chunk_len)\n",
        "    target = torch.LongTensor(batch_size, chunk_len)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, file_len - chunk_len)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = file[start_index:end_index]\n",
        "        inp[bi] = char_tensor(chunk[:-1])\n",
        "        target[bi] = char_tensor(chunk[1:])\n",
        "    inp = Variable(inp)\n",
        "    target = Variable(target)\n",
        "    inp = inp.to(device)\n",
        "    target = target.to(device)\n",
        "    return inp, target\n",
        "\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden(batch_size)\n",
        "    hidden = hidden.to(device)\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[:,c], hidden)\n",
        "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data / chunk_len\n",
        "\n",
        "def save():\n",
        "    save_filename = 'shake1.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "# Initialize models and start training\n",
        "\n",
        "decoder = CharRNN(\n",
        "    27,\n",
        "    hidden_size,\n",
        "    27,\n",
        "    n_layers,\n",
        ")\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "decoder.to(device)\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "try:\n",
        "    print(\"Training for %d epochs...\" % n_epochs)\n",
        "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
        "        loss = train(*random_training_set(chunk_len, batch_size))\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "            print(generate(decoder, 'where', 100, 0.8), '\\n')\n",
        "\n",
        "    print(\"Saving...\")\n",
        "    save()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Saving before quit...\")\n",
        "    save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa4hakOA6ZZZ",
        "outputId": "6587db74-cbc0-4816-bc1d-78f53d153a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 100/2000 [00:53<16:44,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0m 53s (100 5%) 1.7837]\n",
            "where wither my prothers of the strocest bose the romed thim in firtuiing and in the good youch aft the l \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 200/2000 [01:46<16:21,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1m 46s (200 10%) 1.6778]\n",
            "wheres to the worst so to herelyford great be anmy ful but then and fants deseary and he thy brants sir h \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 300/2000 [02:39<15:16,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2m 39s (300 15%) 1.6509]\n",
            "where he arm therefore mother look it nor viny the king dead on say and he hair if this lifes kivecleman  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 400/2000 [03:32<14:34,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3m 32s (400 20%) 1.6097]\n",
            "where has it friends come and rebelt tlerence but and and laywhich to speaking in your bound are i enence \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 500/2000 [04:24<13:38,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4m 24s (500 25%) 1.6140]\n",
            "where decetter my heart and what so like a prithee for him what attend the lovely in his fares it of my w \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 600/2000 [05:17<13:09,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5m 17s (600 30%) 1.6122]\n",
            "where stand out the cries at their tage base and and her trues this doth into thee sir from the friends b \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 700/2000 [06:09<11:21,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6m 9s (700 35%) 1.5885]\n",
            "where me he found gone mancation you age his dear not wirt that in thine every sin be subtle my lord wake \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 800/2000 [07:01<10:42,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7m 1s (800 40%) 1.5761]\n",
            "where not in those and deliver would make she was then fought in show you i percent and this best of men  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 900/2000 [07:53<09:56,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7m 53s (900 45%) 1.5784]\n",
            "where reason near the sake alongd me king one sirrowert you revens cry and thrower sorrows with we will b \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1000/2000 [08:46<09:12,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8m 46s (1000 50%) 1.5995]\n",
            "where king thing and to the suptiture see that the sent forth mine well i he not writees beare but with a \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 1100/2000 [09:34<07:05,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9m 34s (1100 55%) 1.5627]\n",
            "where to will no mine lames thy maid the stayd didst that as that will be dead call awhile in you with my \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1200/2000 [10:20<06:41,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10m 20s (1200 60%) 1.5647]\n",
            "where skils and he sirtreat cheeks read but tain your good and you hope thee prince you less and well wou \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1300/2000 [11:15<06:59,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11m 15s (1300 65%) 1.5668]\n",
            "where was her imonry he head did all suretyce make the married of t but he were wherefore and sportingers \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1400/2000 [12:08<05:13,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12m 8s (1400 70%) 1.5513]\n",
            "where ever and and a for this paint for your high that isue him make of this master edgarlight of some be \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1500/2000 [13:01<04:42,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13m 1s (1500 75%) 1.5483]\n",
            "where of me fury entor speechisgai with my fall breath that for me me the houses for heaven water there s \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1600/2000 [13:54<03:32,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13m 54s (1600 80%) 1.5658]\n",
            "where he this pass much so by three but isly feeds is my lord forband honest her love old exit that i hav \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 1700/2000 [14:48<02:46,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14m 48s (1700 85%) 1.5611]\n",
            "where not and thy father in from where of place part do with this i shall not we prose and lang and thus  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1800/2000 [15:42<01:46,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15m 42s (1800 90%) 1.5521]\n",
            "where and where fair him stay shame rest right this prey beare there poor exeunt of perceive your seporac \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 1900/2000 [16:36<00:51,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16m 36s (1900 95%) 1.5482]\n",
            "where i have all sing the her honour of love ill my still bold for her stay to the leave thee with the he \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [17:29<00:00,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17m 29s (2000 100%) 1.5546]\n",
            "where it say wrong poor under to marles but houset her good speed that reports be the grief thou abolut w \n",
            "\n",
            "Saving...\n",
            "Saved as shake1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = torch.load('shake1.pt')\n",
        "generate(decoder, 'where', 100, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YYJAIpx3-PH9",
        "outputId": "f1ed94ef-b893-4437-e096-d21913b849bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'where what i may how the sight what let here drimmy could for the case i don exit and to the light the he'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss after 2000 epochs is 1.54."
      ],
      "metadata": {
        "id": "GdhIyNKES94p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use GRU RNN"
      ],
      "metadata": {
        "id": "J49uZuEV6upA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.model = model.lower()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        if self.model == \"gru\":\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        elif self.model == \"lstm\":\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        batch_size = input.size(0)\n",
        "        encoded = self.encoder(input)\n",
        "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
        "        output = self.decoder(output.view(batch_size, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def forward2(self, input, hidden):\n",
        "        encoded = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.model == \"lstm\":\n",
        "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
        "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
        "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ],
      "metadata": {
        "id": "ZtjrFUlhwysd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden(1)\n",
        "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
        "\n",
        "    hidden = hidden.to(device)\n",
        "    prime_input = prime_input.to(device)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[:,p], hidden)\n",
        "        \n",
        "    inp = prime_input[:,-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = idx_to_char[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
        "        inp = inp.to(device)\n",
        "\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "7FAgAfKv2J00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"shake.txt\"\n",
        "model=\"gru\"\n",
        "n_epochs=2000\n",
        "print_every=100\n",
        "hidden_size=100\n",
        "n_layers=2\n",
        "learning_rate=0.01\n",
        "chunk_len=200\n",
        "batch_size=100\n",
        "\n",
        "\n",
        "file = raw_dataset\n",
        "file_len = len(file)\n",
        "\n",
        "def random_training_set(chunk_len, batch_size):\n",
        "    inp = torch.LongTensor(batch_size, chunk_len)\n",
        "    target = torch.LongTensor(batch_size, chunk_len)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, file_len - chunk_len)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = file[start_index:end_index]\n",
        "        inp[bi] = char_tensor(chunk[:-1])\n",
        "        target[bi] = char_tensor(chunk[1:])\n",
        "    inp = Variable(inp)\n",
        "    target = Variable(target)\n",
        "    inp = inp.to(device)\n",
        "    target = target.to(device)\n",
        "    return inp, target\n",
        "\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden(batch_size)\n",
        "    hidden = hidden.to(device)\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[:,c], hidden)\n",
        "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data / chunk_len\n",
        "\n",
        "def save():\n",
        "    save_filename = 'shake2.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "# Initialize models and start training\n",
        "\n",
        "decoder = CharRNN(\n",
        "    27,\n",
        "    hidden_size,\n",
        "    27,\n",
        "    model,\n",
        "    n_layers,\n",
        ")\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "decoder.to(device)\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "try:\n",
        "    print(\"Training for %d epochs...\" % n_epochs)\n",
        "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
        "        loss = train(*random_training_set(chunk_len, batch_size))\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "            print(generate(decoder, 'where', 100, 0.8), '\\n')\n",
        "\n",
        "    print(\"Saving...\")\n",
        "    save()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Saving before quit...\")\n",
        "    save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWM1q-nxw09i",
        "outputId": "9e23838f-a72b-4bdf-9cc8-2dccc5682256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 100/2000 [01:33<30:57,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1m 33s (100 5%) 1.7210]\n",
            "where is what chick not why glowing nother to to to me good both to th notsce from was forture scrick tru \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 200/2000 [03:07<28:14,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3m 7s (200 10%) 1.5915]\n",
            "where melest down and men shall be shoil of the soul and the by a mannes to the like of father to dead wh \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 300/2000 [04:43<26:27,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4m 43s (300 15%) 1.5413]\n",
            "where take our be here he do my such for have so and but she general faith indeed the clown in thou fear  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 400/2000 [06:18<26:10,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6m 18s (400 20%) 1.5378]\n",
            "where come withal so and your bear have you a friend my forth which i do so for it is praise thou may tim \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 500/2000 [07:53<23:32,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7m 53s (500 25%) 1.5157]\n",
            "where is the crampst some off and this base and holy the ring the great thednay indeed a man lest to thei \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 600/2000 [09:28<23:17,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9m 28s (600 30%) 1.4886]\n",
            "where my lord shall be megenot dream the valention of mind of man was and from what come of for event how \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 700/2000 [11:03<20:48,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11m 2s (700 35%) 1.4849]\n",
            "where is my bosomsdid able all gloucester auty eyes fie a will whats the world and means for thee his lif \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 800/2000 [12:38<19:37,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12m 38s (800 40%) 1.4720]\n",
            "where sir she not we the basewave yongut to mine eyes if this vice they for a wanton disprisionanishd tim \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 900/2000 [14:13<17:45,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14m 13s (900 45%) 1.4665]\n",
            "where is the lord you do be that young must a high youth valentine that sufter berowne king be with all h \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1000/2000 [15:48<16:20,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15m 48s (1000 50%) 1.4890]\n",
            "where nor she has my lordhave she watch that nome flourishesbreaten be a rest the carbuse i have not men  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 1100/2000 [17:22<14:23,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17m 22s (1100 55%) 1.4913]\n",
            "where is could be a glass but heard the person the field to else i have seems richlaniusarman it content  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1200/2000 [18:52<11:09,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18m 52s (1200 60%) 1.4830]\n",
            "where he shall i hate and that the silkd to dient make her she now exit i may they lies with a man by the \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1300/2000 [20:13<09:37,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20m 13s (1300 65%) 1.4794]\n",
            "where the monamuniep of them what thought hast this this charge to you wakings are get the particulat is  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1400/2000 [21:35<08:40,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21m 35s (1400 70%) 1.4707]\n",
            "where are pervitamine as i swear can levd the lord this lose all jook and bless brutus in my foot and out \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1500/2000 [22:57<07:19,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22m 57s (1500 75%) 1.4732]\n",
            "where is not this good countried and faith for with me to have made his lord in the treason have i am for \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1600/2000 [24:22<05:42,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24m 22s (1600 80%) 1.4679]\n",
            "whereatthat yhard i say brings my sir sir descendage exit he must cheerful simpleit his colours a pole an \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 1700/2000 [25:50<04:26,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25m 50s (1700 85%) 1.4811]\n",
            "where havious world you havethats to him that suffer the cunning to this and shdust andwhose than whom hi \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1800/2000 [27:17<03:11,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27m 17s (1800 90%) 1.4574]\n",
            "where being ere the change him and much thou shalt no westmorle it and things to make him i made the heav \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 1900/2000 [28:44<01:24,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[28m 44s (1900 95%) 1.4961]\n",
            "where no sir of the action what hoit shall be me where is can pursue me and when i be roman thou from my  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [30:07<00:00,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30m 7s (2000 100%) 1.4760]\n",
            "where a selfof the falls from his hand and the duke pedro scived paris pardon that the bosom hearize but  \n",
            "\n",
            "Saving...\n",
            "Saved as shake2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = torch.load('shake2.pt')\n",
        "generate(decoder, 'where', 100, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZZ4rxkWqw8Ro",
        "outputId": "3053d631-e55d-48e8-f8e7-706963e081d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'where i am a man i am sworn up and soldiers for the better fallswe him in her virtues to london that hath'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss after 2000 epochs is 1.45, which is less than the vanilla RNN model. In that case, use GRU can help the model to train faster."
      ],
      "metadata": {
        "id": "aAKWWNh9TLdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use small dataset of Paul Graham Essays "
      ],
      "metadata": {
        "id": "nvl9li-52N46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pg_essay.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "raw_dataset = ' '.join(re.sub('[^A-Za-z ]+','', text).lower().split())\n",
        "# Maps token index to character\n",
        "idx_to_char = list(set(raw_dataset))\n",
        "# Maps character to token index\n",
        "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "# Tokenize the dataset\n",
        "corpus_indices = [char_to_idx[char] for char in raw_dataset]"
      ],
      "metadata": {
        "id": "kBWH8chU2kOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"pg_essay.txt\"\n",
        "model=\"gru\"\n",
        "n_epochs=2000\n",
        "print_every=100\n",
        "hidden_size=100\n",
        "n_layers=2\n",
        "learning_rate=0.01\n",
        "chunk_len=200\n",
        "batch_size=100\n",
        "\n",
        "\n",
        "file = raw_dataset\n",
        "file_len = len(file)\n",
        "\n",
        "def random_training_set(chunk_len, batch_size):\n",
        "    inp = torch.LongTensor(batch_size, chunk_len)\n",
        "    target = torch.LongTensor(batch_size, chunk_len)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, file_len - chunk_len)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = file[start_index:end_index]\n",
        "        inp[bi] = char_tensor(chunk[:-1])\n",
        "        target[bi] = char_tensor(chunk[1:])\n",
        "    inp = Variable(inp)\n",
        "    target = Variable(target)\n",
        "    inp = inp.to(device)\n",
        "    target = target.to(device)\n",
        "    return inp, target\n",
        "\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden(batch_size)\n",
        "    hidden = hidden.to(device)\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[:,c], hidden)\n",
        "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data / chunk_len\n",
        "\n",
        "def save():\n",
        "    save_filename = 'pg.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "# Initialize models and start training\n",
        "\n",
        "decoder = CharRNN(\n",
        "    27,\n",
        "    hidden_size,\n",
        "    27,\n",
        "    model,\n",
        "    n_layers,\n",
        ")\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "decoder.to(device)\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "try:\n",
        "    print(\"Training for %d epochs...\" % n_epochs)\n",
        "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
        "        loss = train(*random_training_set(chunk_len, batch_size))\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "            print(generate(decoder, 'where', 100, 0.8), '\\n')\n",
        "\n",
        "    print(\"Saving...\")\n",
        "    save()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Saving before quit...\")\n",
        "    save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx5snXUY2UQj",
        "outputId": "b26f867f-caec-4d5d-940b-4fd3c238b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 100/2000 [01:31<29:42,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1m 31s (100 5%) 1.5827]\n",
            "where the seincy looks and their ungeith or one a natence in existing i evice to rights programs in veste \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 200/2000 [03:03<27:38,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3m 3s (200 10%) 1.4205]\n",
            "where meeting on the later is this it was they dont iscoop college the day like fand bgsspect but in many \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 300/2000 [04:35<26:52,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4m 35s (300 15%) 1.4184]\n",
            "whereas i nerrea in the most reas that hard are they were founders were ingoot what supportun al seems to \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 400/2000 [06:07<25:12,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6m 7s (400 20%) 1.3583]\n",
            "where as true of the professors in google for the working to partment often a grants than word startup wo \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 500/2000 [07:39<23:29,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7m 38s (500 25%) 1.3389]\n",
            "where no pay but it are couldi paydes and working it one will tend to something the describers which mone \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 600/2000 [09:12<19:27,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9m 12s (600 30%) 1.3191]\n",
            "where the most companies of their schools have or stockbnowerblockquote its trying to advertives its chan \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 700/2000 [10:35<18:06,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10m 35s (700 35%) 1.3279]\n",
            "where the developers wants for it only a round mas ask to work its in be suck the only applery the idea o \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 800/2000 [11:59<17:31,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11m 59s (800 40%) 1.2921]\n",
            "whereor a startup that in the sinds of the bestinclities or with early comes to refut it and more words b \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 900/2000 [13:23<16:12,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13m 23s (900 45%) 1.3235]\n",
            "where we would be technology they dont have to discodiences the two big companyin the night has writing t \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1000/2000 [14:47<13:45,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14m 47s (1000 50%) 1.3139]\n",
            "where would we may look a him that that and all the people and since so predict witz is it you so the bal \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 1100/2000 [16:11<12:44,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16m 11s (1100 55%) 1.3198]\n",
            "whereer on to get for every investments would be find the increasing it that firms the most im must rous  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1200/2000 [17:34<11:22,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17m 34s (1200 60%) 1.3118]\n",
            "where we were being a new jobs which means imost or him they have the number of vcs the whole version to  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1300/2000 [18:59<09:55,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18m 59s (1300 65%) 1.3189]\n",
            "where about designers but is almost such about time or trends that approomption in a company down they co \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1400/2000 [20:22<08:27,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20m 22s (1400 70%) 1.3173]\n",
            "where it works as a company assee a common kind of clust is actually how for little it wouldnt have been  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1500/2000 [21:45<06:59,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21m 45s (1500 75%) 1.3163]\n",
            "where and the more the most compete programmer vcs college is to ahead to conflict in this code indeedebu \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1600/2000 [23:08<05:35,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23m 8s (1600 80%) 1.2970]\n",
            "where it wouldnt intellectual whole invest like special fatal notes a big funded in date these gotten of  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 1700/2000 [24:34<04:22,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24m 34s (1700 85%) 1.3183]\n",
            "where the famous people a buzzeem as well would be take wealth and for a remarket and new big companies o \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1800/2000 [25:59<02:48,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25m 59s (1800 90%) 1.2434]\n",
            "where they sad yet someone market befores now i does the worldial things and like quotes who has to live  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 1900/2000 [27:24<01:33,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27m 24s (1900 95%) 1.2769]\n",
            "where are always friends with by whatever better in something to say you really best velous of essay thin \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [28:48<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[28m 48s (2000 100%) 1.2993]\n",
            "where now as forexample for matnow of the outcome they came and a company we find on the presocriate to p \n",
            "\n",
            "Saving...\n",
            "Saved as pg.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = torch.load('pg.pt')\n",
        "generate(decoder, 'where', 100, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tEY1FogH2ujR",
        "outputId": "a8bbe652-8628-4dd2-e98d-a554165679bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'where a countried to definitely of the many better and when i dont experiment of the otherif a lot profit'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss after 2000 epochs is 1.24. Since this time I also use the \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "same GRU model, the reason for less training loss is becasue the new text dataset is much smaller than shakespare text, which causes the model train faster. "
      ],
      "metadata": {
        "id": "XxCse2iqTeha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgement\n",
        "The code is adpated from: https://github.com/spro/char-rnn.pytorch\n",
        "\n",
        "The Paul Graham dataset is downloaded from: https://www.kaggle.com/krsoninikhil/pual-graham-essays"
      ],
      "metadata": {
        "id": "6MTpiRd3UA_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JYqNXlWKUd5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}